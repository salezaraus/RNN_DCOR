A Distance Correlationâ€“Based Toolkit for Characterizing RNN Effectiveness in Time Series Forecasting

This repository implements the methodology from:

A Distance Correlation-Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series ForecastingChristopher Salazar, Ashis G. Banerjee (University of Washington) îˆ€citeîˆ‚turn1file0îˆ

ğŸ“‹ Highlights

Evaluates internal RNN behavior using distance correlation to track information flow through activation layers.

Diagnoses memory loss: shows how RNNs learn and gradually forget lag structures over layers.

Benchmarks synthetic & real data: AR, MA, ARMA, GARCH processes; ETTh1, OT, Solarâ€‘energy, NASDAQ indices.

Hyperparameter insights: input window size dominates over activation functions or hidden unit count.

ğŸ› ï¸ Features

Modular codebase in my_timeseries_pkg/:

generator.pyâ€‚â€“â€‚Synthetic & realâ€‘data series generator (TSGenerator).

acfs.pyâ€‚â€“â€‚ACF vs. ADCF computations & plotting utilities.

rnn.pyâ€‚â€“â€‚RNN model build, training, activationâ€‘extraction routines.

metrics.pyâ€‚â€“â€‚MSE, SMAPE, WAPE, Relative MSE implementations.

utils.pyâ€‚â€“â€‚I/O helpers (CSV export, results saving).

Orchestration via run_experiments.py: loops over multiple processes, runs simulations, generates plots & CSV summaries.

Configurable via commandâ€‘line flags: series length, window, epochs, hyperparameters.

ğŸš€ Installation

Clone this repo:

git clone https://github.com/salezaraus/RNN_DCOR.git
cd rnn_dc_simulation

Set up a virtual environment (optional but recommended):

python3 -m venv venv
source venv/bin/activate  # on Windows: venv\\Scripts\\activate

Install dependencies:

pip install -r requirements.txt

ğŸ¯ Usage

Run the default suite of experiments (AR(1,5,10,20)):

python run_experiments.py --output_dir results

Key arguments:

--num_runsâ€‚(number of Monte Carlo runs)

--window / --overlap / --horizon (slidingâ€‘window settings)

--epochs / --batch_size / --hidden_units (RNN hyperparameters)

--mean / --std / --frequency / --drift / --omega (TS generator settings)

Results will be saved under results/:

results/figures/â€‚â€“â€‚ACF/ADCF barplots & timeâ€series forecasts

results/summary.csvâ€‚â€“â€‚aggregated MSE, SMAPE, WAPE, Relative MSE per process

ğŸ” Repository Structure

my_timeseries_project/
â”œâ”€â”€ config.py             # (optional) global hyperparameter definitions
â”œâ”€â”€ run_experiments.py    # CLI entryâ€point for experiments
â”œâ”€â”€ requirements.txt      # Python package dependencies
â”œâ”€â”€ my_timeseries_pkg/    # core modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator.py
â”‚   â”œâ”€â”€ acfs.py
â”‚   â”œâ”€â”€ rnn.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ tests/                # (optional) unit tests
    â”œâ”€â”€ test_generator.py
    â”œâ”€â”€ test_acfs.py
    â””â”€â”€ test_metrics.py

ğŸ“– Paper Summary

The paper introduces a distance correlationâ€“based framework to:

Quantify how RNN activation layers capture lag dependencies in time series.

Reveal that RNNs, while learning shortâ€range lags well, lose memory across ~5â€“6 layers, harming forecasts on highâ€lag processes.

Demonstrate RNN struggles on MA and heteroskedastic (GARCH) series.

Visualize model similarities via heatmapsâ€”showing input window size drives performance more than activation or hidden units.

These insights help practitioners preâ€‘assess RNN suitability for new time series, guide hyperparameter choices, and interpret internal network dynamics without exhaustive tuning.

ğŸ“œ Citation

If you use this code, please cite:

Salazar, C. & Banerjee, A. G. â€œA Distance Correlationâ€“Based Approach to Characterize the Effectiveness of Recurrent Neural Networks for Time Series Forecastingâ€. Neurocomputing, 2025.Â 

ğŸ“„ License

This project is licensed under the MIT License. See LICENSE for details.

Developed by Christopherâ€¯Salazar, University of Washington.

